{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zNMD5M6mQ2YY"
   },
   "source": [
    "# RNNs, LSTMs & GRUs\n",
    "\n",
    "#### Description\n",
    "\n",
    "This tutorial was based off a deep learning coursework. The aim was to implement basic RNN cells, as well as LSTMs and GRUs to better understand their inner mechanisms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bWGb-eUeXtex"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "I will be using the Google [*Speech Commands*](https://www.tensorflow.org/tutorials/sequences/audio_recognition) v0.02 [1] dataset. In particular, I will be using a subset of the dataset containing only the words \"one\", \"two\" and \"three\". In this notebook, I will be developing RNNs to classify the respective audio signals into the approriate labels. Rather than working with the raw audio signals, we will be using Mel spectogram representations of the original data. \n",
    "\n",
    "[1] Warden, P. (2018). [Speech commands: A dataset for limited-vocabulary speech recognition](https://arxiv.org/abs/1804.03209). *arXiv preprint arXiv:1804.03209.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAKE SURE THIS POINTS INSIDE THE DATASET FOLDER.\n",
    "dataset_folder = \"../\" # this should change depending on where you have stored the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qt3KzJzBPdHU"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "import librosa\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QKSnqpAJLVwx"
   },
   "outputs": [],
   "source": [
    "class SpeechCommandsDataset(Dataset):\n",
    "    \"\"\"Google Speech Commands dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, split):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the data files.\n",
    "            split    (string): In [\"train\", \"valid\", \"test\"].\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "\n",
    "        self.number_of_classes = len(self.get_classes())\n",
    "\n",
    "        self.class_to_file = defaultdict(list)\n",
    "\n",
    "        self.valid_filenames = self.get_valid_filenames()\n",
    "        self.test_filenames = self.get_test_filenames()\n",
    "\n",
    "        for c in self.get_classes():\n",
    "            file_name_list = sorted(os.listdir(self.root_dir + \"data_speech_commands_v0.02/\" + c))\n",
    "            for filename in file_name_list:\n",
    "                if split == \"train\":\n",
    "                    if (filename not in self.valid_filenames[c]) and (filename not in self.test_filenames[c]):\n",
    "                        self.class_to_file[c].append(filename)\n",
    "                elif split == \"valid\":\n",
    "                    if filename in self.valid_filenames[c]:\n",
    "                        self.class_to_file[c].append(filename)\n",
    "                elif split == \"test\":\n",
    "                    if filename in self.test_filenames[c]:\n",
    "                        self.class_to_file[c].append(filename)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid split name.\")\n",
    "\n",
    "        self.filepath_list = list()\n",
    "        self.label_list = list()\n",
    "        for cc, c in enumerate(self.get_classes()):\n",
    "            f_extension = sorted(list(self.class_to_file[c]))\n",
    "            l_extension = [cc for i in f_extension]\n",
    "            f_extension = [self.root_dir + \"data_speech_commands_v0.02/\" + c + \"/\" + filename for filename in f_extension]\n",
    "            self.filepath_list.extend(f_extension)\n",
    "            self.label_list.extend(l_extension)\n",
    "        self.number_of_samples = len(self.filepath_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.number_of_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = np.zeros((16000, ), dtype=np.float32)\n",
    "\n",
    "        sample_file = self.filepath_list[idx]\n",
    "\n",
    "        sample_from_file = read(sample_file)[1]\n",
    "        sample[:sample_from_file.size] = sample_from_file\n",
    "        sample = sample.reshape((16000, ))\n",
    "        \n",
    "        sample = librosa.feature.mfcc(y=sample, sr=16000, hop_length=512, n_fft=2048).transpose().astype(np.float32)\n",
    "\n",
    "        label = self.label_list[idx]\n",
    "\n",
    "        return sample, label\n",
    "\n",
    "    def get_classes(self):\n",
    "        return ['one', 'two', 'three']\n",
    "\n",
    "    def get_valid_filenames(self):\n",
    "        class_names = self.get_classes()\n",
    "\n",
    "        class_to_filename = defaultdict(set)\n",
    "        with open(self.root_dir + \"data_speech_commands_v0.02/validation_list.txt\", \"r\") as fp:\n",
    "            for line in fp:\n",
    "                clean_line = line.strip().split(\"/\")\n",
    "\n",
    "                if clean_line[0] in class_names:\n",
    "                    class_to_filename[clean_line[0]].add(clean_line[1])\n",
    "\n",
    "        return class_to_filename\n",
    "\n",
    "    def get_test_filenames(self):\n",
    "        class_names = self.get_classes()\n",
    "\n",
    "        class_to_filename = defaultdict(set)\n",
    "        with open(self.root_dir + \"data_speech_commands_v0.02/testing_list.txt\", \"r\") as fp:\n",
    "            for line in fp:\n",
    "                clean_line = line.strip().split(\"/\")\n",
    "\n",
    "                if clean_line[0] in class_names:\n",
    "                    class_to_filename[clean_line[0]].add(clean_line[1])\n",
    "\n",
    "        return class_to_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vx8ptirGKa9u"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = SpeechCommandsDataset(dataset_folder,\n",
    "                                      \"train\")\n",
    "valid_dataset = SpeechCommandsDataset(dataset_folder,\n",
    "                                      \"valid\")\n",
    "\n",
    "test_dataset = SpeechCommandsDataset(dataset_folder,\n",
    "                                     \"test\")\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "num_epochs = 20\n",
    "valid_every_n_steps = 20\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJwOesOQOSh9"
   },
   "source": [
    "### Question 1:  Finalise the LSTM and GRU cells by completing the missing code\n",
    "\n",
    "You are allowed to use nn.Linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQu9Yxfy-Wqj"
   },
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "    \n",
    "        self.x2h = nn.Linear(input_size, 4*hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, 4*hidden_size, bias=bias)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "        if hx is None:\n",
    "            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n",
    "            hx = (hx, hx)\n",
    "            \n",
    "        hx, cx = hx # packing both hidden states\n",
    "        preact = self.x2h(input) + self.h2h(hx)\n",
    "        i, o, f, c_tilde = torch.chunk(preact, 4, dim=-1)\n",
    "        # Activations\n",
    "        i, o, f, c_tilde = i.sigmoid(), o.sigmoid(), f.sigmoid(), c_tilde.tanh()\n",
    "        cy = torch.mul(f, cx) + torch.mul(i, c_tilde)\n",
    "        hy = torch.mul(o, cy.tanh())\n",
    "        return (hy, cy)\n",
    "\n",
    "class BasicRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True, nonlinearity=\"tanh\"):\n",
    "        super(BasicRNNCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.nonlinearity = nonlinearity\n",
    "        if self.nonlinearity not in [\"tanh\", \"relu\"]:\n",
    "            raise ValueError(\"Invalid nonlinearity selected for RNN.\")\n",
    "\n",
    "        self.x2h = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "        \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "            \n",
    "    def forward(self, input, hx=None):\n",
    "        if hx is None:\n",
    "            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n",
    "\n",
    "        activation = getattr(nn.functional, self.nonlinearity)\n",
    "        hy = activation(self.x2h(input) + self.h2h(hx))\n",
    "\n",
    "        return hy\n",
    "\n",
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "\n",
    "        self.x2h = nn.Linear(input_size, 2*hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, 2*hidden_size, bias=bias)\n",
    "        self.x2r = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.h2r = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "        if hx is None:\n",
    "            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n",
    "        gates = (self.x2r(input) + self.h2r(hx)).sigmoid()\n",
    "        r, z = torch.chunk(gates, 2, dim=-1)\n",
    "        # Activations\n",
    "        ht_tilde = (self.h2h(torch.mul(r, hx) +self.x2h(input))).tanh()\n",
    "        hy = torch.mul(1 - z, h) + torch.mul(z, ht_tilde) # compute hidden unit\n",
    "        return hy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalise the RNNModel and BidirRecurrentModel\n",
    "\n",
    "Note that there are serveral different ways that one can implement a bi-directional recurrent neural network. In this task I implement bidirectional RNNs with one RNN going each way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQu9Yxfy-Wqj"
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, mode, input_size, hidden_size, num_layers, bias, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.output_size = output_size\n",
    "        self.rnn_cell_list = nn.ModuleList([])\n",
    "        \n",
    "        if mode == 'LSTM':\n",
    "            self.rnn_cell_list.append(LSTMCell(input_size, hidden_size, bias=True))\n",
    "            for l in range(1, num_layers):\n",
    "                self.rnn_cell_list.append(LSTMCell(hidden_size, hidden_size, bias=True))\n",
    "\n",
    "        elif mode == 'GRU':\n",
    "            self.rnn_cell_list.append(GRUCell(input_size, hidden_size, bias=True))\n",
    "            for l in range(1, num_layers):\n",
    "                self.rnn_cell_list.append(GRUCell(hidden_size, hidden_size, bias=True)) \n",
    "        \n",
    "        elif mode == 'RNN_TANH':\n",
    "            self.rnn_cell_list.append(BasicRNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh'))\n",
    "            for l in range(1, num_layers):\n",
    "                self.rnn_cell_list.append(BasicRNNCell(hidden_size, hidden_size, bias=True, nonlinearity='tanh'))\n",
    "                \n",
    "        elif mode == 'RNN_RELU':\n",
    "            self.rnn_cell_list.append(BasicRNNCell(input_size, hidden_size, bias=True, nonlinearity='relu'))\n",
    "            for l in range(1, num_layers):\n",
    "                self.rnn_cell_list.append(BasicRNNCell(hidden_size, hidden_size, bias=True, nonlinearity='relu'))\n",
    "\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid RNN mode selected.\")\n",
    "\n",
    "\n",
    "        self.att_fc = nn.Linear(self.hidden_size, 1)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, input, hx=None):\n",
    "\n",
    "        outs = []\n",
    "        h0 = [None] * self.num_layers if hx is None else list(hx)\n",
    "        \n",
    "        # In this forward pass we want to create our RNN from the rnn cells,\n",
    "        # ..taking the hidden states from the final RNN layer and passing these \n",
    "        # ..through our fully connected layer (fc).\n",
    "        \n",
    "        # The multi-layered RNN should be able to run when the mode is either \n",
    "        # .. LSTM, GRU, RNN_TANH or RNN_RELU.\n",
    "        \n",
    "        h_prev = input\n",
    "        for n in range(self.num_layers):\n",
    "            h_new = torch.zeros(input.size(0), input.size(1), self.hidden_size)\n",
    "            ht = h0[n]\n",
    "            for j in range(input.size(1)): # for each sequence step\n",
    "                ht = self.rnn_cell_list[n](h_prev[:,j,:], ht) # forward step for each cell\n",
    "                h_new[:,j,:] = ht if self.mode != 'LSTM' else ht[0]\n",
    "            h_prev = h_new\n",
    "\n",
    "        out = h_prev[:,-1,:].squeeze()\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "class BidirRecurrentModel(nn.Module):\n",
    "    def __init__(self, mode, input_size, hidden_size, num_layers, bias, output_size):\n",
    "        super(BidirRecurrentModel, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.rnn_cell_list = nn.ModuleList()\n",
    "        self.rnn_cell_list_rev = nn.ModuleList()\n",
    "        \n",
    "        if mode == 'LSTM':\n",
    "            self.rnn_cell_list.append(LSTMCell(input_size, hidden_size, bias=True))\n",
    "            for l in range(1, num_layers):\n",
    "                self.rnn_cell_list.append(LSTMCell(hidden_size, hidden_size, bias=True))\n",
    "            self.rnn_cell_list_rev.append(LSTMCell(input_size, hidden_size, bias=True))\n",
    "            for l in range(1, num_layers):\n",
    "                self.rnn_cell_list_rev.append(LSTMCell(hidden_size, hidden_size, bias=True))\n",
    "\n",
    "        elif mode == 'GRU':\n",
    "            self.rnn_cell_list.append(GRUCell(input_size, hidden_size, bias=True))\n",
    "            for l in range(1, num_layers):\n",
    "                self.rnn_cell_list.append(GRUCell(hidden_size, hidden_size, bias=True))\n",
    "            self.rnn_cell_list_rev.append(GRUCell(input_size, hidden_size, bias=True))\n",
    "            for l in range(1, num_layers):\n",
    "                self.rnn_cell_list_rev.append(GRUCell(hidden_size, hidden_size, bias=True))     \n",
    "        \n",
    "        elif mode == 'RNN_TANH':\n",
    "            self.rnn_cell_list.append(BasicRNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh'))\n",
    "            for l in range(1, num_layers):\n",
    "                self.rnn_cell_list.append(BasicRNNCell(hidden_size, hidden_size, bias=True, nonlinearity='tanh'))\n",
    "            self.rnn_cell_list_rev.append(BasicRNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh'))\n",
    "            for l in range(1, num_layers):\n",
    "                self.rnn_cell_list_rev.append(BasicRNNCell(hidden_size, hidden_size, bias=True, nonlinearity='tanh'))\n",
    "                \n",
    "        elif mode == 'RNN_RELU':\n",
    "            self.rnn_cell_list.append(BasicRNNCell(input_size, hidden_size, bias=True, nonlinearity='relu'))\n",
    "            for l in range(1, num_layers):\n",
    "                self.rnn_cell_list.append(BasicRNNCell(hidden_size, hidden_size, bias=True, nonlinearity='relu'))\n",
    "            self.rnn_cell_list_rev.append(BasicRNNCell(input_size, hidden_size, bias=True, nonlinearity='relu'))\n",
    "            for l in range(1, num_layers):\n",
    "                self.rnn_cell_list_rev.append(BasicRNNCell(hidden_size, hidden_size, bias=True, nonlinearity='relu'))\n",
    "     \n",
    "    def forward(self, input, hx=None):\n",
    "        \n",
    "        # In this forward pass we want to create our Bidirectional RNN from the rnn cells,\n",
    "        # .. taking the hidden states from the final RNN layer with their reversed counterparts\n",
    "        # .. before concatening these and running them through the fully connected layer (fc)\n",
    "        \n",
    "        # The multi-layered RNN should be able to run when the mode is either \n",
    "        # .. LSTM, GRU, RNN_TANH or RNN_RELU.\n",
    "        \n",
    "        outs = []\n",
    "        outs_rev = []\n",
    "        \n",
    "        X = list(input.permute(1, 0, 2))\n",
    "        X_rev = list(input.permute(1, 0, 2))\n",
    "        X_rev.reverse()\n",
    "        hi = [None] * self.num_layers if hx is None else list(hx)\n",
    "        hi_rev = [None] * self.num_layers if hx is None else list(hx)\n",
    "        for j in range(self.num_layers):\n",
    "            hx = hi[j]\n",
    "            hx_rev = hi_rev[j]\n",
    "            for i in range(input.shape[1]):\n",
    "                hx = self.rnn_cell_list[j](X[i], hx)\n",
    "                X[i] = hx if self.mode != 'LSTM' else hx[0]\n",
    "                hx_rev = self.rnn_cell_list_rev[j](X_rev[i], hx_rev)\n",
    "                X_rev[i] = hx_rev if self.mode != 'LSTM' else hx_rev[0]\n",
    "        outs = X \n",
    "        outs_rev = X_rev \n",
    "        out = outs[-1].squeeze()\n",
    "        out_rev = outs_rev[0].squeeze()\n",
    "        out = torch.cat((out, out_rev), 1)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQu9Yxfy-Wqj"
   },
   "outputs": [],
   "source": [
    "\n",
    "seq_dim, input_dim = train_dataset[0][0].shape\n",
    "output_dim = 3\n",
    "\n",
    "hidden_dim = 32\n",
    "layer_dim = 3\n",
    "bias = True\n",
    "\n",
    "### Change the code below to try running different models:\n",
    "model = RNNModel(\"LSTM\", input_dim, hidden_dim, layer_dim, bias, output_dim)\n",
    "# model = BidirRecurrentModel(\"LSTM\", input_dim, hidden_dim, layer_dim, bias, output_dim)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_list = []\n",
    "iter = 0\n",
    "max_v_accuracy = 0\n",
    "reported_t_accuracy = 0\n",
    "max_t_accuracy = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (audio, labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            audio = Variable(audio.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            audio = Variable(audio.view(-1, seq_dim, input_dim))\n",
    "            labels = Variable(labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(audio)\n",
    "        if outputs.dim() == 1:\n",
    "            outputs = outputs.view(1, -1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            loss.cuda()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "        iter += 1\n",
    "\n",
    "        if iter % valid_every_n_steps == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for audio, labels in valid_loader:\n",
    "                if torch.cuda.is_available():\n",
    "                    audio = Variable(audio.view(-1, seq_dim, input_dim).cuda())\n",
    "                else:\n",
    "                    audio = Variable(audio.view(-1, seq_dim, input_dim))\n",
    "\n",
    "                outputs = model(audio)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "\n",
    "            v_accuracy = 100 * correct // total\n",
    "            \n",
    "            is_best = False\n",
    "            if v_accuracy >= max_v_accuracy:\n",
    "                max_v_accuracy = v_accuracy\n",
    "                is_best = True\n",
    "\n",
    "            if is_best:\n",
    "                for audio, labels in test_loader:\n",
    "                    if torch.cuda.is_available():\n",
    "                        audio = Variable(audio.view(-1, seq_dim, input_dim).cuda())\n",
    "                    else:\n",
    "                        audio = Variable(audio.view(-1, seq_dim, input_dim))\n",
    "\n",
    "                    outputs = model(audio)\n",
    "\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                    total += labels.size(0)\n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "                    else:\n",
    "                        correct += (predicted == labels).sum()\n",
    "\n",
    "                t_accuracy = 100 * correct // total\n",
    "                reported_t_accuracy = t_accuracy\n",
    "\n",
    "            print('Iteration: {}. Loss: {}. V-Accuracy: {}  T-Accuracy: {}'.format(iter, loss.item(), v_accuracy, reported_t_accuracy))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RNN tutorial and coursework.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "0fc1a29019f730bf8c2ab645f5a89e95fe9c0c901b8c24076e8d2ed30bd20ab0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
